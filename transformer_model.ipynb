{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "print(tf.__version__)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import shutil\n",
    "import zipfile\n",
    "import itertools\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"./data/bwd_sample5000.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    \"\"\" Converts the unicode file to ascii \"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # adding a start and an end token to the sentence\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples=None):\n",
    "    \"\"\" create dataset Clean the sentences and Return word pairs in the format: [equation, integral] \"\"\"\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq, intgr = create_dataset(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(inp, sequence_length):\n",
    "    \"\"\" word to index \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(inp)\n",
    "    sequences = tokenizer.texts_to_sequences(inp)\n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post', maxlen=sequence_length, truncating='post')\n",
    "    return  sequences, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 512\n",
    "\n",
    "# Tokenize each word into index and return the tokenized list and tokenizer\n",
    "X , X_tokenizer = tokenize(eq)\n",
    "Y,  Y_tokenizer = tokenize(intgr)\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize by frequency\n",
    "X_tokenizer.word_index['<start>']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size # add 1 for 0 padding \n",
    "input_vocab_size = len(X_tokenizer.word_index) + 1 \n",
    "output_vocab_size = len(Y_tokenizer.word_index)+ 1\n",
    "\n",
    "print(\"input_vocab_size : \", input_vocab_size)\n",
    "print(\"output_vocab_size : \" ,output_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build transformer \n",
    "- building in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### only for model test\n",
    "sequence_length = 512\n",
    "input_vocabulary_size = 1000\n",
    "output_vocabulary_size = 1000\n",
    "###\n",
    "d_model = 512\n",
    "embedding_size = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "depth = d_model // num_heads\n",
    "dff = 2048\n",
    "rate = 0.1\n",
    "training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = layers.Input(shape=(sequence_length, ), name=\"encoder_input\")\n",
    "x_en = encoder_input\n",
    "x_en = layers.Embedding(\n",
    "    input_vocabulary_size,\n",
    "    embedding_size,\n",
    ")(x_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "positioning encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_en = layers.Add()([x_en * tf.math.sqrt(tf.cast(d_model, tf.float32)), positional_encoding(sequence_length, d_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_en = layers.Dropout(rate=rate)(x_en, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, depth):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "    \n",
    "        assert d_model % self.num_heads == 0\n",
    "    \n",
    "    \n",
    "        self.wq = tf.keras.layers.Dense(depth)\n",
    "        self.wk = tf.keras.layers.Dense(depth)\n",
    "        self.wv = tf.keras.layers.Dense(depth)\n",
    "        self.attention = tf.keras.layers.Attention(use_scale=True,)\n",
    "    \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, mask):\n",
    "        for i in range(num_heads):\n",
    "            q_i = self.wq(inputs[0])\n",
    "            k_i = self.wk(inputs[1])\n",
    "            v_i = self.wv(inputs[2])\n",
    "            self_attention = self.attention([q_i, v_i, k_i])\n",
    "            if i == 0:\n",
    "                concat_attention = tf.concat([self_attention], axis=2)\n",
    "            else:\n",
    "                concat_attention = tf.concat([concat_attention, self_attention], axis=2)      \n",
    "        self_attention = self.dense(concat_attention)\n",
    "\n",
    "        return self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(d_model, num_heads, depth, input_layer, layer, training):\n",
    "    #multi-head self attention \n",
    "    self_attention = MultiHeadAttention(d_model, num_heads, depth)([input_layer, input_layer, input_layer], mask=None)\n",
    "    self_attenton_output = layers.Dropout(rate=0.1)(self_attention, training=training)\n",
    "    \n",
    "    #layer norm1\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6, name=\"layer\" + str(layer) + \"_\" + \"en_layer_norm1\")(self_attenton_output + input_layer)\n",
    "    \n",
    "    # feed forward\n",
    "    ffn1 = tf.keras.layers.Dense(dff, activation='relu')(layernorm1)\n",
    "    ffn2 = tf.keras.layers.Dense(d_model, name=\"layer\" + str(layer) + \"_\" + \"en_feed_forward\")(ffn1)\n",
    "    ffn2_output = layers.Dropout(rate=0.1)(ffn2, training=training)\n",
    "    \n",
    "    #layer norm2\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6, name=\"layer\" + str(layer) + \"_\" + \"en_layer_norm2\")(ffn2_output+layernorm1)\n",
    "    return layernorm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_input_layer = x_en\n",
    "for i in range(num_layers):\n",
    "    en_output_layer = encoder_layer(d_model, num_heads, depth, en_input_layer, i, training)\n",
    "    en_input_layer = en_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = en_output_layer\n",
    "en_model = models.Model(encoder_input, encoder_output)\n",
    "#en_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_graph = keras.utils.plot_model(en_model, show_shapes=True, to_file=\"transformer_encoder_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = layers.Input(shape=(sequence_length, ), name=\"decoder_input\")\n",
    "x_de = decoder_input\n",
    "x_de = layers.Embedding(\n",
    "    output_vocabulary_size,\n",
    "    embedding_size,\n",
    ")(x_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "positioning encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_de = layers.Add()([x_de * tf.math.sqrt(tf.cast(d_model, tf.float32)), positional_encoding(sequence_length, d_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_de = layers.Dropout(rate=rate)(x_de, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(input_layer, encoder_output, num_heads, layer, training):\n",
    "    #multi-head self attention \n",
    "    self_attention_de = MultiHeadAttention(d_model, num_heads, depth)([input_layer, input_layer, input_layer], mask=None)\n",
    "    self_attenton_de_output = layers.Dropout(rate=0.1)(self_attention_de, training=training)\n",
    "    \n",
    "    #layer norm1\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6, name=\"layer\" + str(layer) + \"_\" + \"de_layer_norm1\")(self_attenton_de_output + input_layer)\n",
    "    \n",
    "    #encoder decoder attention\n",
    "    en_de_attention = MultiHeadAttention(d_model, num_heads, depth)([input_layer,encoder_output, encoder_output], mask=None)\n",
    "    en_de_attenton_output = layers.Dropout(rate=0.1)(en_de_attention, training=training)\n",
    "    \n",
    "    #layer norm2\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6, name=\"layer\" + str(layer) + \"_\" + \"de_layer_norm2\")(en_de_attenton_output + layernorm1)\n",
    "    \n",
    "    # feed forward\n",
    "    ffn1 = tf.keras.layers.Dense(dff, activation='relu')(layernorm2)\n",
    "    ffn2 = tf.keras.layers.Dense(d_model, name=\"layer\" + str(layer) + \"_\" + \"de_feed_forward\")(ffn1)\n",
    "    ffn2_output = layers.Dropout(rate=0.1)(ffn2, training=training)\n",
    "    \n",
    "    #layer norm3\n",
    "    layernorm3 = layers.LayerNormalization(epsilon=1e-6, name=\"layer\" + str(layer) + \"_\" + \"de_layer_norm3\")(ffn2_output + layernorm2)\n",
    "    return layernorm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_input_layer = x_de\n",
    "for i in range(num_layers):\n",
    "    de_output_layer = decoder_layer(de_input_layer, encoder_output, num_heads, i, training)\n",
    "    de_input_layer = de_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = de_output_layer\n",
    "model = models.Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = keras.utils.plot_model(model, show_shapes=False, to_file=\"transformer_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show structure of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file=\"model.png\")\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy']\n",
    "             )\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['loss'], label='loss')\n",
    "plt.plot(model.history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(model.history.history['accuracy'], label='accuracy')\n",
    "plt.plot(model.history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbolic_math",
   "language": "python",
   "name": "symbolic_math"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
